import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torchvision.utils import save_image

# ─── Hyperparameters ──────────────────────────────────────────────────────────
DEVICE       = "cuda" if torch.cuda.is_available() else "cpu"
DATA_PATH    = "corrosionset2/train"
BATCH_SIZE   = 64
IMG_SIZE     = 64
IMG_CHANNELS = 3
LATENT_DIM   = 100
EMBED_DIM    = 50    # size of label embedding
NUM_CLASSES  = len(os.listdir(DATA_PATH))
EPOCHS       = 100
LR           = 2e-4
BETAS        = (0.5, 0.999)
SAVE_DIR     = "cgan_images"
os.makedirs(SAVE_DIR, exist_ok=True)

# ─── Dataset & Dataloader ─────────────────────────────────────────────────────
transform = transforms.Compose([
    transforms.Resize((IMG_SIZE,IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*IMG_CHANNELS, [0.5]*IMG_CHANNELS),
])
dataset = ImageFolder(root=DATA_PATH, transform=transform)
loader  = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)

# ─── Generator ─────────────────────────────────────────────────────────────────
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.label_emb = nn.Embedding(NUM_CLASSES, EMBED_DIM)
        self.net = nn.Sequential(
            # input is (LATENT_DIM + EMBED_DIM)
            nn.Linear(LATENT_DIM+EMBED_DIM, 256 * 8 * 8),
            nn.BatchNorm1d(256*8*8),
            nn.ReLU(True),
            # reshape to (256,8,8)
            View((256,8,8)),
            # upsample to (128,16,16)
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            # upsample to (64,32,32)
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            # upsample to (IMG_CHANNELS,64,64)
            nn.ConvTranspose2d(64, IMG_CHANNELS, 4, 2, 1, bias=False),
            nn.Tanh()
        )

    def forward(self, z, labels):
        # z: (B, LATENT_DIM), labels: (B,)
        l = self.label_emb(labels)              # (B,EMBED_DIM)
        x = torch.cat([z, l], dim=1)            # (B, LATENT_DIM+EMBED_DIM)
        return self.net(x)

# ─── Discriminator ─────────────────────────────────────────────────────────────
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.label_emb = nn.Embedding(NUM_CLASSES, EMBED_DIM)
        self.img_net = nn.Sequential(
            # input (IMG_CHANNELS,64,64)
            nn.Conv2d(IMG_CHANNELS, 64, 4, 2, 1, bias=False),  # (64,32,32)
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),           # (128,16,16)
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),          # (256,8,8)
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Flatten(),                                      # (B,256*8*8)
        )
        # combine image features + label embedding
        self.final = nn.Sequential(
            nn.Linear(256*8*8 + EMBED_DIM, 1),
            nn.Sigmoid(),
        )

    def forward(self, img, labels):
        feat = self.img_net(img)
        l    = self.label_emb(labels)
        x    = torch.cat([feat, l], dim=1)
        return self.final(x)

# ─── Utility: reshape view ─────────────────────────────────────────────────────
class View(nn.Module):
    def __init__(self, shape):
        super().__init__()
        self.shape = shape
    def forward(self, x):
        return x.view(x.size(0), *self.shape)

# ─── Initialize ────────────────────────────────────────────────────────────────
gen  = Generator().to(DEVICE)
disc = Discriminator().to(DEVICE)
optG = torch.optim.Adam(gen.parameters(),  lr=LR, betas=BETAS)
optD = torch.optim.Adam(disc.parameters(), lr=LR, betas=BETAS)
adversarial_loss = nn.BCELoss()

# ─── Training Loop ─────────────────────────────────────────────────────────────
for epoch in range(1, EPOCHS+1):
    for i, (imgs, labels) in enumerate(loader):
        bsz = imgs.size(0)
        real = torch.full((bsz,1), 1.0, device=DEVICE)
        fake = torch.full((bsz,1), 0.0, device=DEVICE)

        imgs   = imgs.to(DEVICE)
        labels = labels.to(DEVICE)

        # --- Train Discriminator
        z = torch.randn(bsz, LATENT_DIM, device=DEVICE)
        gen_imgs = gen(z, labels)
        # real
        out_real = disc(imgs, labels)
        loss_real = adversarial_loss(out_real, real)
        # fake
        out_fake = disc(gen_imgs.detach(), labels)
        loss_fake = adversarial_loss(out_fake, fake)
        lossD = (loss_real + loss_fake) / 2

        optD.zero_grad(); lossD.backward(); optD.step()

        # --- Train Generator
        z = torch.randn(bsz, LATENT_DIM, device=DEVICE)
        gen_imgs = gen(z, labels)
        out_gen  = disc(gen_imgs, labels)
        lossG = adversarial_loss(out_gen, real)  # want the fake to be classified as real

        optG.zero_grad(); lossG.backward(); optG.step()

        if i % 100 == 0:
            print(f"Epoch {epoch}/{EPOCHS} Batch {i}/{len(loader)} "
                  f"LossD: {lossD.item():.4f} LossG: {lossG.item():.4f}")

    # save a grid of generated samples conditioned on each class
    with torch.no_grad():
        fixed_z = torch.randn(NUM_CLASSES, LATENT_DIM, device=DEVICE)
        fixed_labels = torch.arange(NUM_CLASSES, device=DEVICE)
        samples = gen(fixed_z, fixed_labels).cpu()
        save_image(samples, f"{SAVE_DIR}/epoch_{epoch}.png", nrow=NUM_CLASSES, normalize=True)

# ─── Save weights ──────────────────────────────────────────────────────────────
torch.save(gen.state_dict(),  "cgan_generator.pth")
torch.save(disc.state_dict(), "cgan_discriminator.pth")
